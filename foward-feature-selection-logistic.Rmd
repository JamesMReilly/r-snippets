---
title: "Forward Feature Selection - Animal Scat"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

In this exercise I explore the [Animal Scat](https://topepo.github.io/caret/data-sets.html#animal-scat-data) dataset, provided through the caret. I repeat the forward feature selection from a previous exercise but use a logistic regression instead of a linear regression. 

###1) Dataset selection and intial view 
```{r open dependencies, message=FALSE}
#open required libraries
library(caret)
library(ggplot2)
library(dplyr)
library(forecast)
library(fastDummies)
library(e1071)
```

This dataset can be loaded through the below command if caret is installed

```{r display the initial data}
data(scat)

scat <- fastDummies::dummy_cols(scat, select_columns = "Species")
scat <- scat %>% filter(complete.cases(.))
nrows <- nrow(scat)
ncols <- ncol(scat)
head(scat)
```
Our dataset has `r nrows` records of animal scat and `r ncols` variables for each record. We can see from the `head` that the variables are in numbers and booleans (through one hot encoding). Through a multi-variate logistic regression, we can try to predict the `Species` (our dependent variable) with the remaining predictor variables. Our first task will be determining which variables is the best predictor.

For review, our previous model was the following:

```{r previous model}
set.seed(1) # set the seed so that the partition is always the same
df.train.index <- sample(c(1:nrows), nrows* 0.7) #use 70% of the data as a training set
selected.vars <- c("Length", "Diameter", "Taper", "TI", "Mass", "d13C", "d15N", "CN", "ropey", "segmented")

knn.vars <- c("Species", selected.vars)
bobcat.vars <- c("Species_bobcat", selected.vars)
coyote.vars <- c("Species_coyote", selected.vars)
gray_fox.vars <- c("Species_gray_fox", selected.vars)

df.train <- scat[df.train.index, knn.vars]
df.valid <- scat[-df.train.index, knn.vars]

df.class <- scat[-df.train.index, c("Species")]

options(scipen = 999)

bobcat.train <- scat[df.train.index, bobcat.vars]
bobcat.test <- scat[-df.train.index, bobcat.vars]
coyote.train <- scat[df.train.index, coyote.vars]
coyote.test <- scat[-df.train.index, coyote.vars]
gray.fox.train <- scat[df.train.index, gray_fox.vars]
gray.fox.test <- scat[-df.train.index, gray_fox.vars]
```


```{r fit the model}
base.knn <- train(Species ~ d13C + Mass + CN + Length, data = df.train, method = "knn")
confusionMatrix(predict(base.knn, df.valid), df.valid$Species)
```

Using the same set of initial predictors, training set, and test set, I will re create the model using a logistic regression

###2) First round selection

Since logistic regression is for binary classification, we need to answer to simplify our original question from "What species is this sample from?" To start I will begin with whether or not a sample is from a bobcat.


```{r logistic first round}
base.log <- glm(Species_bobcat ~ 1, data = bobcat.train, family = "binomial")

bobcat.log.forward <- step(base.log, scope = ~ Length + Diameter + Taper + TI + Mass + d13C + d15N + CN + ropey + segmented, steps = 1, direction = "forward")

summary(bobcat.log.forward)

bobcat.predictions <- predict(bobcat.log.forward, bobcat.test, type = "response")
confusionMatrix(factor(as.numeric(bobcat.predictions > 0.5)), factor(bobcat.test$Species_bobcat))
```

The first feature selection using the logistic model was `CN`, with an accuracy of prediction of 67.86%. I will repeat feature selection using only the best 5 predictors from the previous step `segmented`, `d13C`, `d15N`, `Diameter`, and `TI` 

```{r logistic second round}
bobcat.log.forward <- step(bobcat.log.forward, scope = ~ Diameter + TI + d13C + d15N + CN + segmented, steps = 1, direction = "forward")

summary(bobcat.log.forward)

bobcat.predictions <- predict(bobcat.log.forward, bobcat.test, type = "response")
confusionMatrix(factor(as.numeric(bobcat.predictions > 0.5)), factor(bobcat.test$Species_bobcat))
```

Our second predictor is `d13C`, with an accuracy of 82.14%. The accuracy improved by including the second feature.

Finally repeating selection until no improvement is observed

```{r repeat selection}
bobcat.log.forward <- step(bobcat.log.forward, scope = ~ Diameter + TI + d13C + d15N + CN + segmented, direction = "forward")

summary(bobcat.log.forward)

bobcat.predictions <- predict(bobcat.log.forward, bobcat.test, type = "response")
confusionMatrix(factor(as.numeric(bobcat.predictions > 0.5)), factor(bobcat.test$Species_bobcat))
```

Our final model predicts Species.bobcat using `CN` + `d13C` + `d15N` + `Diameter`

It had an accuracy of 82%. I will repeat this process quickly for Gray Fox and Coyote classification

```{r coyote selection}
base.log <- glm(Species_coyote ~ 1, data = coyote.train, family = "binomial")

coyote.log.forward <- step(base.log, scope = ~ Length + Diameter + Taper + TI + Mass + d13C + d15N + CN + ropey + segmented, direction = "forward")

summary(coyote.log.forward)

coyote.predictions <- predict(coyote.log.forward, coyote.test, type = "response")
confusionMatrix(factor(as.numeric(coyote.predictions > 0.5)), factor(coyote.test$Species_coyote))
```

Our coyote model has a 96.43% accuracy with the predictors: `d15N` + `d13C` + `Diameter` + `segmented` + `Length`

```{r gray fox selection}
base.log <- glm(Species_gray_fox ~ 1, data = gray.fox.train, family = "binomial")

gray.fox.log.forward <- step(base.log, scope = ~ Length + Diameter + Taper + TI + Mass + d13C + d15N + CN + ropey + segmented, direction = "forward")

summary(gray.fox.log.forward)

gray.fox.predictions <- predict(gray.fox.log.forward, gray.fox.test, type = "response")
confusionMatrix(factor(as.numeric(gray.fox.predictions > 0.5)), factor(gray.fox.test$Species_gray_fox))
```

Our gray fox model had an accuracy of 82.14% using the predictors `Mass` + `CN` + `Taper` + `d13C`

```{r ensemble the models}
predictions <- data.frame(df.class,  bobcat.predictions, coyote.predictions, gray.fox.predictions)

bobcatIsPredicted <- function(bobcat, coyote, gray_fox) {
  bobcat > coyote && bobcat > gray_fox
}

coyoteIsPredicted <- function(bobcat, coyote, gray_fox) {
  coyote > bobcat && coyote > gray_fox
}

addClass <- function(bobcat, coyote, fox) {
  if(bobcatIsPredicted(bobcat, coyote, fox)) {
    "bobcat"
  } else if (coyoteIsPredicted(bobcat, coyote, fox)) {
    "coyote"
  } else {
    "gray_fox"
  }
}

predictions <- predictions %>% rowwise() %>% mutate(Predicted.Class = addClass(bobcat.predictions, coyote.predictions, gray.fox.predictions))

predictions$Predicted.Class <- factor(predictions$Predicted.Class)

confusionMatrix(predictions$Predicted.Class, predictions$df.class)

```