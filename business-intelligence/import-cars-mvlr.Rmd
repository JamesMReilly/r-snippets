---
title: "Multivariate Linear Regression for Imported Car Prices"
author: "Jim Reilly"
output:
  pdf_document: default
  html_notebook: default
---


```{r, message=FALSE, include=FALSE}
library(forecast)
library(caret)
```

## Data

Load the data and add column labels

```{r}
df <- read.csv(file = "./data/imports-85.data", header = FALSE, na.strings = "?")
columnNames <- c(
  "symboling",        
  "normalizedlosses",
  "make",
  "fueltype",        
  "aspiration",       
  "numdoors",     
  "bodystyle",    
  "drivewheels",     
  "engine-location",  
  "wheelbase", 
  "length",      
  "width",          
  "height",           
  "curbweight",      
  "enginetype",     
  "num-ofcylinders", 
  "enginesize",
  "fuelsystem",     
  "bore",     
  "stroke",           
  "compressionratio",
  "horsepower",
  "peakrpm",      
  "citympg",        
  "highwaympg",      
  "price")

colnames(df) <- columnNames
```


## Exploration

First to identify missing data, our dataset represents a missing value with the `?` character. Identifying the number of `?` values and where they occur should guide our filtering strategy. I've converted `?` to NA in the `read.csv` expression above.

```{r}
sapply(df, function(x) sum(is.na(x)))
```

Several columns are missing *some* values, however the `normalized-losses` column is missing a considerable amount. I will drop the column and subsequently remove any rows with missing values after.

```{r}
df.preprocessed <- df[,-2]
df.preprocessed <- na.omit(df.preprocessed)
```

After removing the column 2 and accepting only the complete cases of the remaining columns, we actually only have to remove 12 rows which are missing at least one value. This leaves us with 193 cars to build a model. 

## Preprocessing

Since the numerical variables have differening scales, I will normailze all columns to a range between 0.0 - 1.0

```{r normalize}
#find what columns are numeric
numerics <- grep(paste(c("numeric", "integer"), collapse="|"), lapply(df.preprocessed,class))

numerics <- head(numerics, -1)

#define the normalization function
normalize <- function(x) { (x - min(x)) / (max(x) - min(x)) }

#apply the function to the numeric columns
df.preprocessed[numerics]<- apply(df.preprocessed[,numerics], MARGIN = 2, FUN = normalize) 
```

## Model training

To remove a potential source of bias in our model's performance, I will split the data into a train and test set using 80% of the available data in training. I will also only use the numerical variables before splitting the factors into dummies because of the limited number of data points.

```{r}
nrows <- nrow(df.preprocessed)
set.seed(47) # set the seed so that the partition is always the same
df.train.index <- sample(c(1:nrows), nrows * 0.8) #use 80% of the data as a training set
selectedVars <- c("symboling", "wheelbase", "length", "width", "height", "curbweight", "enginesize", "bore", "stroke", "compressionratio", "horsepower", "peakrpm", "citympg", "highwaympg")

lm.vars <- c("price", selectedVars)

df.train <- df.preprocessed[df.train.index, lm.vars]
df.valid <- df.preprocessed[-df.train.index, lm.vars]

lm <- train(price ~ ., data = df.train, method = "lm")


actuals_pred <- predict(lm, newdata = df.valid)
mape <- mean(abs((actuals_pred - df.valid$price))/df.valid$price)
plot(y = df.valid$price, x = actuals_pred, main = "Forecasted Price vs. Actual", xlab="Actual Price", ylab = "Forecasted Price")

```


Using all predictors, we have a model with a mean absolute percent error of `r mape`