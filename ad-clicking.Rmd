---
title: "Ad Clicking"
author: "Jim Reilly"
date: "February 18, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r clear the global env}
rm(list=ls())
```

```{r message=FALSE}
library(dplyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(caret)
library(randomForest)
```

###1) Loading the data

Our data is a .csv with 1,000,000 entries. We must also convert a couple columns to factors.

```{r load the data}
ads <- read.csv(file = 'data/criteo_ad_click_sampled.csv', header = FALSE)
```

###2) Transform the data

We need to transform some of our columns before we can build regressions. I will convert the label (column V1) to a factor so that we can build a binary classifier later.

```{r mutate the dataset}
ads <- ads %>% mutate(V1 = factor(V1))

head(ads)
```

###3) Exploratory Analysis

We will look at some preliminary features to decide what variables might be good predictors and to better understand the available data

```{r summary of the set}
clicks <- summary(ads$V1)
```
We have `r clicks[1]` no clicks and `r clicks[2]` clicks. Since we care more about the clicks than the no clicks we might consider some oversampling as described in the text.

We also have a very large quantity of data. I will produce my visualizations off of a sample for performance concerns.

```{r random sample with seed}
set.seed(4747) # set seed for reproducability

ads.sample.index <- sample(1:nrow(ads), 10000)
ads.big.sample.index <- sample(1:nrow(ads), 100000)

ads.sample <- ads[ads.sample.index, ]
```

```{r visualization of V2}
v2.plot <- ggplot(ads.sample, aes(x = V1, y = V2))

v2.plot + geom_boxplot()
v2.plot + geom_jitter()
```

```{r visualization of V3}
v3.plot <- ggplot(ads.sample, aes(x = V1, y = V3))

v3.plot + geom_boxplot()
v3.plot + geom_jitter()
```

```{r visualization of V4}
v4.plot <- ggplot(ads.sample, aes(x = V1, y = V4))

v4.plot + geom_boxplot()
v4.plot + geom_jitter()
```

```{r visualization of V5}
v5.plot <- ggplot(ads.sample, aes(x = V1, y = V5))

v5.plot + geom_boxplot()
v5.plot + geom_jitter()
```


```{r visualization of V6}
v6.plot <- ggplot(ads.sample, aes(x = V1, y = V6))

v6.plot + geom_boxplot()
v6.plot + geom_jitter()
```

```{r visualization of V7}
v7.plot <- ggplot(ads.sample, aes(x = V1, y = V7))

v7.plot + geom_boxplot()
v7.plot + geom_jitter()
```

```{r visualization of V8}
v8.plot <- ggplot(ads.sample, aes(x = V1, y = V8))

v8.plot + geom_boxplot()
v8.plot + geom_jitter()
```

```{r visualization of V9}
v9.plot <- ggplot(ads.sample, aes(x = V1, y = V9))

v9.plot + geom_boxplot()
v9.plot + geom_jitter()
```

```{r visualization of V10}
v10.plot <- ggplot(ads.sample, aes(x = V1, y = V10))

v10.plot + geom_boxplot()
v10.plot + geom_jitter()
```

```{r visualization of V11}
v11.plot <- ggplot(ads.sample, aes(x = V1, y = V11))

v11.plot + geom_boxplot()
v11.plot + geom_jitter()
```

```{r visualization of V12}
v12.plot <- ggplot(ads.sample, aes(x = V1, y = V12))

v12.plot + geom_boxplot()
v12.plot + geom_jitter()
```

```{r visualization of V13}
v13.plot <- ggplot(ads.sample, aes(x = V1, y = V13))

v13.plot + geom_boxplot()
v13.plot + geom_jitter()
```

```{r visualization of V14}
v14.plot <- ggplot(ads.sample, aes(x = V1, y = V14))

v14.plot + geom_boxplot()
v14.plot + geom_jitter()
```

```{r visualization of V15}
v15.plot <- ggplot(ads.sample, aes(x = V1, y = V15))

v15.plot + geom_boxplot()
v15.plot + geom_jitter()
```

```{r visualization of V16}
v16.plot <- ggplot(ads.sample, aes(x = V1, y = V16))

v16.plot + geom_boxplot()
v16.plot + geom_jitter()
```

```{r visualization of V17}
v17.plot <- ggplot(ads.sample, aes(x = V1, y = V17))

v17.plot + geom_boxplot()
v17.plot + geom_jitter()
```

```{r visualization of V18}
v18.plot <- ggplot(ads.sample, aes(x = V1, y = V18))

v18.plot + geom_boxplot()
v18.plot + geom_jitter()
```

```{r visualization of V19}
v19.plot <- ggplot(ads.sample, aes(x = V1, y = V19))

v19.plot + geom_boxplot()
v19.plot + geom_jitter()
```

```{r visualization of V20}
v20.plot <- ggplot(ads.sample, aes(x = V1, y = V20))

v20.plot + geom_boxplot()
v20.plot + geom_jitter()
```

```{r visualization of V21}
v21.plot <- ggplot(ads.sample, aes(x = V1, y = V21))

v21.plot + geom_boxplot()
v21.plot + geom_jitter()
```

```{r visualization of V22}
v22.plot <- ggplot(ads.sample, aes(x = V1, y = V22))

v22.plot + geom_boxplot()
v22.plot + geom_jitter()
```

V5-V10 and V14 show a pattern where there seems to be a higher median value for the `0` target vs. the `1`. I would assume that these will be identified as better predictors than some of the other choices. Of the factors, V15 - V18 seem to have a skew in the number of points belonging to each class for some factor values. The remaining factors have too low variance between the two targets or if there is a major difference the sample size in that factor appears small on the jitter plot (which is significant for 100,000 records). In order to best reason with the factors individually I convert them to dummy variables with a 0 or 1 encoding.

```{r ads dummies}
#ads.sample <- fastDummies::dummy_cols(ads.sample, select_columns = c("V15", "V16", "V17", "V18", "V19", "V20", "V21", "V22"))
ads.sample <- ads[ads.big.sample.index, ]
```

###4) Build a model (Classification Tree)
I attempt a classification tree as a modeling method. I find a deeper tree by specifying a higher cp value in the parting method so that I can see which predictors are best.

```{r test}
default.ct <- rpart(V1 ~ ., data = ads.sample, method = "class")
prp(default.ct, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
default.ct.pred <- predict(default.ct, ads.sample, type = "class")
confusionMatrix(default.ct.pred, ads.sample$V1)

default.ct.pred <- predict(default.ct, ads, type = "class")
confusionMatrix(default.ct.pred, ads$V1)

deeper.ct <- rpart(V1 ~ ., data = ads.sample, method = "class", cp = 0.001, minsplit = 1)
prp(deeper.ct, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
```

In order to reduce the dimension and simplify the problem, I use a classification tree to identify the strongest predictors. A classification tree divides the dataset into "pure" (meaning all with the same target value) subsets by dividing the records based on a single predictor value. Each node represents a split in the dataset based on one of those predictors, the better the dataset can be divided into pure halves, the higher up the node is in the tree.

Classification trees are unstable however, because they depend highly on the selected sample used to train. I will use the random forest method to reduce the instability in the model, which generates lots of trees based on different folds of the sample. Because the random forest method is computationally intensive, I first reduce the dimension by selecting only the best predictors from the previous tree, V7, V12, V10, V19, V6, V14, V3, and V9.

```{r calculate the random forest}
#random forest dimension reduction
ads.sample.reduced <- ads.sample %>% select(V1, V7, V12, V10, V17, V19, V6, V14, V3, V9)

ads.reduced.sample.rf <- randomForest(V1 ~ ., data = ads.sample.reduced, ntree = 500, mtry = 4, nodesize = 5, importance = TRUE)
ads.reduced.sample.rf.pred <- predict(ads.reduced.sample.rf, ads.sample, type = "class")
confusionMatrix(ads.reduced.sample.rf.pred, ads.sample$V1)
```

I get a really good fit to the sample set with random forest.

```{r test the entire set}
ads.rf.pred <- predict(ads.reduced.sample.rf, ads, type = "class")
confusionMatrix(ads.rf.pred, ads$V1)
```
My random forest built on 10% of the data extrapolated on the entire set had an accuracy of 77.6%. By using a subset of the data in training and the random forest method with 500 trees, I can be more confident that I am not overfitting the noise in the training set and that my accuracy is not an extension of that overfitting.

Unfortunately a random forest cannot be visualized in the same sense as a single tree so regarding my best predictors I know only that it is some combination of V7, V12, V10, V19, V6, V14, V3, and V9.

In order to improve my model I could use a random forest over the entire dataset, which would be computationally complex. The random forest I have generated above took about 10 minutes to finish calculation. This could introduce some performance constraints in some modeling applications. In the end I found that this method was able to quickly identify good performing predictors and simple to make sense out of a dataset that I otherwise know little about.
